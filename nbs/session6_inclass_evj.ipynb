{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Benchmark classification on ```cifar-10```\n",
    "\n",
    "This notebook builds on what we were doing last week with the handwritten digits from the MNIST dataset.\n",
    "\n",
    "This week, we're working with another famous dataset in computer vision and image processing research - [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path tools\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# data loader\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classificatio models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7343d4b5",
   "metadata": {},
   "source": [
    "We're going to load the data using a function from the library ```TensorFlow```, which we'll be looking at in more detail next week. \n",
    "\n",
    "For now, we're just using it to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#return training data and training data labels, and test data and test data labelse. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b588be73",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f4316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# its a four-dimentional numpy.array. The four numbers: 50000, 32, 32, 3 => 32x32 pixels, 3 colour channels, 50000 images of this sort. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd02fcbe",
   "metadata": {},
   "source": [
    "Unfortunately, this version of the data set doesn't have explict labels, so we need to create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cdcf5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "#the labels doesnÃ¸t have names, only a number refering to the names listed alphabetically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \n",
    "          \"automobile\", \n",
    "          \"bird\", \n",
    "          \"cat\", \n",
    "          \"deer\", \n",
    "          \"dog\", \n",
    "          \"frog\", \n",
    "          \"horse\", \n",
    "          \"ship\", \n",
    "          \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all the data to greyscale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5391f3",
   "metadata": {},
   "source": [
    "In the following cell, I'm converting all of my images to greyscale and then making a ```numpy``` array at the end.\n",
    "\n",
    "Notice that I'm using something funky here called *[list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn images into greyscale using list comprehensions. \n",
    "X_train_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train])\n",
    "X_test_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])\n",
    "#list comprehension = can do the same as a for loop, but in a shorter way (I think). \n",
    "# The list comprehension turns everything into a list, so you don't have to append something to an empty list as we often do in for loops. \n",
    "# sometimes list comprehensions decrease readibility compared to for loops, but here it increases readability (it becomes clear that the thing happening in the two lines are identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18717f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_grey.shape # now we have a 3D image, because we have only one colour channel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9703dbdc",
   "metadata": {},
   "source": [
    "Then, we're going to do some simple scaling by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the pixel values in each image smaller (between 0 and 1 instead of between 0 and 255)\n",
    "# This helps models convert faster and get better results, because the weights and biases, the model has to learn are smaller values. \n",
    "X_train_scaled = (X_train_grey)/255.0\n",
    "X_test_scaled = (X_test_grey)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c141a5e2",
   "metadata": {},
   "source": [
    "Next, we're going to reshape this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshaping images in training data\n",
    "nsamples, nx, ny = X_train_scaled.shape # the .shape returns the number of samples (5000) and the number of values on the x and y axes (32 and 32)\n",
    "X_train_dataset = X_train_scaled.reshape((nsamples,nx*ny)) \n",
    "# reshaping: we want only two values: the number of samples (50000) and 32x32 (=1024) \n",
    "# --> the images are each 'flattened down' to one string of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deb7b12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1024)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reshaping images in test data\n",
    "nsamples, nx, ny = X_test_scaled.shape\n",
    "X_test_dataset = X_test_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65740572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bdea84",
   "metadata": {},
   "source": [
    "We define our Logistic Regression classifier as we have done previously. You'll notice that I've set a lot of different parameters here - you can learn more in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.25153370\n",
      "Epoch 3, change: 0.12777784\n",
      "Epoch 4, change: 0.10543609\n",
      "convergence after 5 epochs took 14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.4s finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"none\",\n",
    "                        tol=0.1, # if the model is not improving when the weights are changed by this value, it should stop (?)\n",
    "                        verbose=True, # a flag\n",
    "                        solver=\"saga\",\n",
    "                        multi_class=\"multinomial\").fit(X_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc10cdb4",
   "metadata": {},
   "source": [
    "We can then print our classification report, using the label names that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.34      0.37      0.36      1000\n",
      "  automobile       0.37      0.40      0.38      1000\n",
      "        bird       0.26      0.21      0.23      1000\n",
      "         cat       0.21      0.15      0.18      1000\n",
      "        deer       0.25      0.21      0.23      1000\n",
      "         dog       0.30      0.30      0.30      1000\n",
      "        frog       0.28      0.33      0.30      1000\n",
      "       horse       0.31      0.32      0.31      1000\n",
      "        ship       0.34      0.40      0.37      1000\n",
      "       truck       0.40      0.45      0.42      1000\n",
      "\n",
      "    accuracy                           0.31     10000\n",
      "   macro avg       0.31      0.31      0.31     10000\n",
      "weighted avg       0.31      0.31      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels) #using label names defined ealier on\n",
    "print(report)\n",
    "# the model is not performing that great (there are around 30% chance that the model predicts the image correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f6d9b4",
   "metadata": {},
   "source": [
    "I've set a couple of different parameters here - you can see more in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "**NB!** This will take a long time to run! On the 32 CPU machine on UCloud, this takes around 30 seconds per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30872956\n",
      "Validation score: 0.133000\n",
      "Iteration 2, loss = 2.15971661\n",
      "Validation score: 0.239200\n",
      "Iteration 3, loss = 2.02581278\n",
      "Validation score: 0.265200\n",
      "Iteration 4, loss = 1.97076182\n",
      "Validation score: 0.281800\n",
      "Iteration 5, loss = 1.93555578\n",
      "Validation score: 0.302600\n",
      "Iteration 6, loss = 1.90926190\n",
      "Validation score: 0.315600\n",
      "Iteration 7, loss = 1.89160286\n",
      "Validation score: 0.318800\n",
      "Iteration 8, loss = 1.87500641\n",
      "Validation score: 0.322200\n",
      "Iteration 9, loss = 1.86730610\n",
      "Validation score: 0.316800\n",
      "Iteration 10, loss = 1.85845283\n",
      "Validation score: 0.321200\n",
      "Iteration 11, loss = 1.84549829\n",
      "Validation score: 0.331400\n",
      "Iteration 12, loss = 1.83590762\n",
      "Validation score: 0.328600\n",
      "Iteration 13, loss = 1.82908945\n",
      "Validation score: 0.331400\n",
      "Iteration 14, loss = 1.82320985\n",
      "Validation score: 0.330600\n",
      "Iteration 15, loss = 1.81056794\n",
      "Validation score: 0.343400\n",
      "Iteration 16, loss = 1.80707784\n",
      "Validation score: 0.338400\n",
      "Iteration 17, loss = 1.79877427\n",
      "Validation score: 0.339800\n",
      "Iteration 18, loss = 1.79244407\n",
      "Validation score: 0.351000\n",
      "Iteration 19, loss = 1.78417279\n",
      "Validation score: 0.348800\n",
      "Iteration 20, loss = 1.78163463\n",
      "Validation score: 0.363800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42,\n",
    "                    hidden_layer_sizes=(64, 10), # two hidden layers - often its smart for the layers to become smaller and smaller (but not always)\n",
    "                    learning_rate=\"adaptive\", \n",
    "                    early_stopping=True, # we want the machine to stop early, if the performance dosn't improve any more\n",
    "                    verbose=True,\n",
    "                    max_iter=20).fit(X_train_dataset, y_train) # after 20 iterations (runs of the data) it should stop.\n",
    "\n",
    "# the adaptive: starts out quessing and learning fast, but later it should slow the learning down. \n",
    "# looking at the text below, we can see that the loss function is redecusing for each iteration. \n",
    "## model\n",
    "# every time the model is run through training data, it calculates loss score\n",
    "# validation data: the model shuffles the training data after every run through and takes 10% of the data which it uses to validate the results calculating an accuracy.\n",
    "# the model runs through the data 20 times and calculate the accuracy from the validation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e489977e",
   "metadata": {},
   "source": [
    "Lastly, we can get our classification report as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.38      0.41      0.40      1000\n",
      "  automobile       0.40      0.49      0.44      1000\n",
      "        bird       0.26      0.34      0.30      1000\n",
      "         cat       0.28      0.11      0.16      1000\n",
      "        deer       0.27      0.26      0.27      1000\n",
      "         dog       0.33      0.34      0.34      1000\n",
      "        frog       0.28      0.29      0.28      1000\n",
      "       horse       0.45      0.39      0.42      1000\n",
      "        ship       0.44      0.44      0.44      1000\n",
      "       truck       0.42      0.47      0.44      1000\n",
      "\n",
      "    accuracy                           0.35     10000\n",
      "   macro avg       0.35      0.35      0.35     10000\n",
      "weighted avg       0.35      0.35      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)\n",
    "\n",
    "#this neural network model gives a slightly increase in f1-score compared to the logistic regression classifier (accuracy increased from 0.32 to 0.35)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5067ab",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Take the code outlined in this notebook and turn it into two separate Python scripts, one which performs Logistic Regression classification and one which uses the MLPClassifier on the ```Cifar10``` dataset.\n",
    "\n",
    "Try to use the things we've spoken about in clas\n",
    "- Requirements.txt\n",
    "- Virtual environment\n",
    "- Setup scripts\n",
    "- Argparse\n",
    "\n",
    "This task is [Assignment 2 for Visual Analytics](https://classroom.github.com/a/KLVvny7d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785fa1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
